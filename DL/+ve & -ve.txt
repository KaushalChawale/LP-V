
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.datasets import imdb

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

data = np.concatenate((X_train, X_test), axis=0)
label = np.concatenate((y_train, y_test), axis=0)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

print("Review is", X_train[0])

print("Review label is", y_train[0])

vocab = imdb.get_word_index()

print(list(vocab.items())[:10])
print("y_train distribution:", np.bincount(y_train))
print("y_test distribution:", np.bincount(y_test))

def vectorize(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1
    return results

print("Categories:", np.unique(label))
print("Number of unique words:", len(np.unique(np.hstack(data))))

length = [len(i) for i in data]
print("Average Review length:", np.mean(length))
print("Standard Deviation:", round(np.std(length)))

index = imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = " ".join([reverse_index.get(i - 3, "#") for i in data[0]])
print("Decoded review sample:", decoded[:100] + "...")

data_vectorized = vectorize(data)
label = np.array(label).astype("float32")


import matplotlib.pyplot as plt
import seaborn as sns
labelDF = pd.DataFrame({'label': label})
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=labelDF)
plt.title('Distribution of Positive and Negative Reviews')
plt.xlabel('Sentiment (0=Negative, 1=Positive)')
plt.ylabel('Count')
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_vectorized, label, test_size=0.20, random_state=1)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

from keras.utils import to_categorical
from keras import models
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint

model = models.Sequential()

model.add(layers.Dense(50, activation="relu", input_shape=(10000,)))

model.add(layers.Dropout(0.3))

model.add(layers.Dense(25, activation="relu"))

model.add(layers.Dropout(0.3))

model.add(layers.Dense(1, activation="sigmoid"))

model.summary()

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

early_stopping = EarlyStopping(monitor='val_loss', patience=3)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

history = model.fit(
    X_train,
    y_train,
    epochs=20,
    batch_size=512,
    validation_split=0.2,
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.3f}")
print(f"Test loss: {test_loss:.3f}")

predictions = model.predict(X_test)
predictions = [1 if x > 0.5 else 0 for x in predictions]


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.figure(figsize=(15, 6))


plt.subplot(1, 2, 1)
plt.plot(epochs, acc, 'bo-', label='Training accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

cm = confusion_matrix(y_test, predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print(classification_report(y_test, predictions))

def prepare_review(review, word_index, max_words=10000):
    words = review.lower().split()

    integer_sequence = []
    for word in words:
        if word in word_index and word_index[word] < max_words:
            integer_sequence.append(word_index[word] + 3)
        else:
            integer_sequence.append(2)

    vectorized = np.zeros((1, max_words))
    for i in integer_sequence:
        if i < max_words:
            vectorized[0, i] = 1

    return vectorized

word_index = imdb.get_word_index()

positive_review = "This movie was fantastic! I really enjoyed the plot and the acting was superb."
negative_review = "This was a terrible waste of time. The plot was boring and the characters were poorly developed."

positive_vectorized = prepare_review(positive_review, word_index)
negative_vectorized = prepare_review(negative_review, word_index)

positive_prediction = model.predict(positive_vectorized)[0][0]
negative_prediction = model.predict(negative_vectorized)[0][0]

print(f"Positive review prediction: {positive_prediction:.4f} ({'Positive' if positive_prediction > 0.5 else 'Negative'})")
print(f"Negative review prediction: {negative_prediction:.4f} ({'Positive' if negative_prediction > 0.5 else 'Negative'})")